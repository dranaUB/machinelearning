{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e91f8582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eb9a05ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: numexpr in c:\\users\\devendra\\anaconda3\\lib\\site-packages (2.8.7)\n",
      "Collecting numexpr\n",
      "  Downloading numexpr-2.10.0-cp311-cp311-win_amd64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: numpy>=1.19.3 in c:\\users\\devendra\\anaconda3\\lib\\site-packages (from numexpr) (1.26.4)\n",
      "Downloading numexpr-2.10.0-cp311-cp311-win_amd64.whl (97 kB)\n",
      "   ---------------------------------------- 0.0/97.0 kB ? eta -:--:--\n",
      "   ---- ----------------------------------- 10.2/97.0 kB ? eta -:--:--\n",
      "   ---------------- ----------------------- 41.0/97.0 kB 991.0 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 97.0/97.0 kB 1.1 MB/s eta 0:00:00\n",
      "Installing collected packages: numexpr\n",
      "Successfully installed numexpr-2.10.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --user --upgrade numexpr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7c5eb4f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: bottleneck in c:\\users\\devendra\\anaconda3\\lib\\site-packages (1.3.7)\n",
      "Collecting bottleneck\n",
      "  Downloading Bottleneck-1.3.8-cp311-cp311-win_amd64.whl.metadata (8.1 kB)\n",
      "Requirement already satisfied: numpy in c:\\users\\devendra\\anaconda3\\lib\\site-packages (from bottleneck) (1.26.4)\n",
      "Downloading Bottleneck-1.3.8-cp311-cp311-win_amd64.whl (110 kB)\n",
      "   ---------------------------------------- 0.0/110.1 kB ? eta -:--:--\n",
      "   ---------------------------------------- 0.0/110.1 kB ? eta -:--:--\n",
      "   -------------- ------------------------ 41.0/110.1 kB 667.8 kB/s eta 0:00:01\n",
      "   ---------------------------------------- 110.1/110.1 kB 1.3 MB/s eta 0:00:00\n",
      "Installing collected packages: bottleneck\n",
      "Successfully installed bottleneck-1.3.8\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install --user --upgrade bottleneck"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5f14439a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Unnamed: 0                                               text  label\n",
      "0           0      i just feel really helpless and heavy hearted      4\n",
      "1           1  ive enjoyed being able to slouch about relax a...      0\n",
      "2           2  i gave up my internship with the dmrg and am f...      4\n",
      "3           3                         i dont know i feel so lost      0\n",
      "4           4  i am a kindergarten teacher and i am thoroughl...      4\n"
     ]
    }
   ],
   "source": [
    "df=pd.read_csv('text.csv')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9e129e49",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_name=['id','text','label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fc4a8830",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                               text  label\n",
      "0   0      i just feel really helpless and heavy hearted      4\n",
      "1   1  ive enjoyed being able to slouch about relax a...      0\n",
      "2   2  i gave up my internship with the dmrg and am f...      4\n",
      "3   3                         i dont know i feel so lost      0\n",
      "4   4  i am a kindergarten teacher and i am thoroughl...      4\n"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('text.csv', names=col_name,header=0)\n",
    "\n",
    "# Display the DataFrame\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af2802b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello World This is a test \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def remove_unwanted_characters(text):\n",
    "    # Remove any character that is not a letter or basic punctuation\n",
    "    return re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "\n",
    "# Example usage\n",
    "text = \"Hello, World! This is a test: 1234.\"\n",
    "cleaned_text = remove_unwanted_characters(text)\n",
    "print(cleaned_text)  # Output: Hello World This is a test \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "95709085",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "def convert_to_lowercase(text):\n",
    "    # Convert text to lowercase\n",
    "    return text.lower()\n",
    "\n",
    "text=\"HELLO WORLD\"\n",
    "print(convert_to_lowercase(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a33cd8e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['hello', 'world']\n"
     ]
    }
   ],
   "source": [
    "def tokenize(text):\n",
    "     return text.split()\n",
    "text=\"hello world\"\n",
    "\n",
    "print(tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b1798d83-6ef9-4579-bf00-7693092fef91",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "41910705",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0f659d81-900c-470d-a981-79e0b90cc23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d78656c1-6d1c-4657-8d65-ef19a0481da8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from nltk.stem import WordNetLemmatizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c406ad51-5d3c-4203-b16f-9a8729f1d5a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Devendra\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Devendra\\AppData\\Roaming\\nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1d7b8d4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sample', 'text']\n"
     ]
    }
   ],
   "source": [
    "def remove_stop_words(tokens):\n",
    "    return [token for token in tokens if token not in stopwords.words('english')]\n",
    "\n",
    "tokens = ['this', 'is', 'a', 'sample', 'text']\n",
    "cleaned_tokens = remove_stop_words(tokens)\n",
    "print(cleaned_tokens)    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "4c7d918b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run\n"
     ]
    }
   ],
   "source": [
    "stemmer = PorterStemmer()\n",
    "print(stemmer.stem('running'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "dbbfd209",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['run', 'run', 'runner']\n"
     ]
    }
   ],
   "source": [
    "lemmatizer = WordNetLemmatizer()\n",
    "def stem_and_lem_words(tokens):\n",
    "    return [lemmatizer.lemmatize(stemmer.stem(token)) for token in tokens ]\n",
    "\n",
    "tokens = ['running','runs','runner']\n",
    "\n",
    "print(stem_and_lem_words(tokens))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6485f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['feel', 'realli', 'helpless', 'heavi', 'heart']\n"
     ]
    }
   ],
   "source": [
    "def clean_text(text):\n",
    "    \n",
    "    text1=re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "    text2=text1.lower()\n",
    "    \n",
    "    tokens=text2.split()\n",
    "    tokens1=[token for token in tokens if token not in stopwords.words('english')]\n",
    "    tokens2=[lemmatizer.lemmatize(stemmer.stem(token)) for token in tokens1 ]\n",
    "    return tokens2   \n",
    "\n",
    "print(clean_text('i just feel really helpless and heavy hearted '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "94a3990b",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text']=df['text'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "b149a785",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                               text  label  \\\n",
      "0   0      i just feel really helpless and heavy hearted      4   \n",
      "1   1  ive enjoyed being able to slouch about relax a...      0   \n",
      "2   2  i gave up my internship with the dmrg and am f...      4   \n",
      "3   3                         i dont know i feel so lost      0   \n",
      "4   4  i am a kindergarten teacher and i am thoroughl...      4   \n",
      "\n",
      "                                        cleaned_text  \n",
      "0             [feel, realli, helpless, heavi, heart]  \n",
      "1  [ive, enjoy, abl, slouch, relax, unwind, frank...  \n",
      "2         [gave, internship, dmrg, feel, distraught]  \n",
      "3                           [dont, know, feel, lost]  \n",
      "4  [kindergarten, teacher, thoroughli, weari, job...  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "649c28c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['cleaned_text_str']=df['cleaned_text'].apply(lambda x:' '.join(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "a2802ff3-67f9-46b9-a877-cb882ff86822",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   id                                               text  label  \\\n",
      "0   0      i just feel really helpless and heavy hearted      4   \n",
      "1   1  ive enjoyed being able to slouch about relax a...      0   \n",
      "2   2  i gave up my internship with the dmrg and am f...      4   \n",
      "3   3                         i dont know i feel so lost      0   \n",
      "4   4  i am a kindergarten teacher and i am thoroughl...      4   \n",
      "\n",
      "                                        cleaned_text  \\\n",
      "0             [feel, realli, helpless, heavi, heart]   \n",
      "1  [ive, enjoy, abl, slouch, relax, unwind, frank...   \n",
      "2         [gave, internship, dmrg, feel, distraught]   \n",
      "3                           [dont, know, feel, lost]   \n",
      "4  [kindergarten, teacher, thoroughli, weari, job...   \n",
      "\n",
      "                                    cleaned_text_str  \n",
      "0                   feel realli helpless heavi heart  \n",
      "1  ive enjoy abl slouch relax unwind frankli need...  \n",
      "2               gave internship dmrg feel distraught  \n",
      "3                                dont know feel lost  \n",
      "4  kindergarten teacher thoroughli weari job take...  \n"
     ]
    }
   ],
   "source": [
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "75f6211d-a2dc-40dd-a813-0ea047330ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "a2aa8daa-6a7b-471a-a1fb-88d38a779c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_matrix = tfidf_vectorizer.fit_transform(df['cleaned_text_str'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "de2cd79c-f1a5-4839-bffd-92b909a55f60",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 19556)\t0.4697183772447977\n",
      "  (0, 19617)\t0.6521702031668551\n",
      "  (0, 19757)\t0.49425547133784503\n",
      "  (0, 36704)\t0.3203821994538096\n",
      "  (0, 15265)\t0.08429381653218043\n",
      "  (1, 45306)\t0.13602947859031855\n",
      "  (1, 18016)\t0.15520184817008886\n",
      "  (1, 30749)\t0.16134774585819547\n",
      "  (1, 25982)\t0.23093480888778822\n",
      "  (1, 4657)\t0.14525442757330853\n",
      "  (1, 15605)\t0.16554525951637108\n",
      "  (1, 42732)\t0.14736578758511945\n",
      "  (1, 25151)\t0.18251128830505012\n",
      "  (1, 14690)\t0.27105902389445846\n",
      "  (1, 47684)\t0.29539191719315316\n",
      "  (1, 13758)\t0.1718340812584999\n",
      "  (1, 2377)\t0.1624090392030929\n",
      "  (1, 49641)\t0.1687691082470566\n",
      "  (1, 25143)\t0.17107426179590782\n",
      "  (1, 30547)\t0.1465514448122089\n",
      "  (1, 16393)\t0.2739497796513694\n",
      "  (1, 48110)\t0.33280464088052064\n",
      "  (1, 37180)\t0.19885044226911344\n",
      "  (1, 41357)\t0.3874497518194024\n",
      "  (1, 127)\t0.17700922096003924\n",
      "  :\t:\n",
      "  (416807, 18172)\t0.20455612387942423\n",
      "  (416807, 8978)\t0.2016259027547792\n",
      "  (416807, 41699)\t0.17577867563126776\n",
      "  (416807, 40486)\t0.19263452351022195\n",
      "  (416807, 39943)\t0.16589104526960916\n",
      "  (416807, 30379)\t0.1706990947006335\n",
      "  (416807, 13396)\t0.14897842167094444\n",
      "  (416807, 40513)\t0.17862914893030576\n",
      "  (416807, 38448)\t0.1454529522785791\n",
      "  (416807, 17343)\t0.13988909259152932\n",
      "  (416807, 33619)\t0.10542754566293337\n",
      "  (416807, 45403)\t0.12919344066542301\n",
      "  (416807, 50548)\t0.11622547159963746\n",
      "  (416807, 127)\t0.1382399468461683\n",
      "  (416807, 15265)\t0.025571172464572203\n",
      "  (416808, 10171)\t0.3718028157989529\n",
      "  (416808, 22277)\t0.53426848576985\n",
      "  (416808, 42228)\t0.3651591945782718\n",
      "  (416808, 43853)\t0.408764019326053\n",
      "  (416808, 33619)\t0.23297520368365987\n",
      "  (416808, 45644)\t0.21491188051134252\n",
      "  (416808, 45310)\t0.22899485099281947\n",
      "  (416808, 45306)\t0.2347611912561405\n",
      "  (416808, 42732)\t0.2543255197505715\n",
      "  (416808, 15265)\t0.056507519698983634\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3926f017-c7c9-46c5-b11d-2d78cbab5aef",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 160. GiB for an array with shape (416809, 51666) and data type float64",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[48], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m tfidf_df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(tfidf_matrix\u001b[38;5;241m.\u001b[39mtoarray(), columns\u001b[38;5;241m=\u001b[39mtfidf_vectorizer\u001b[38;5;241m.\u001b[39mget_feature_names_out())\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_compressed.py:1050\u001b[0m, in \u001b[0;36m_cs_matrix.toarray\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1048\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m order \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   1049\u001b[0m     order \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_swap(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcf\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m-> 1050\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_toarray_args(order, out)\n\u001b[0;32m   1051\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mc_contiguous \u001b[38;5;129;01mor\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous):\n\u001b[0;32m   1052\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mOutput array must be C or F contiguous\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\scipy\\sparse\\_base.py:1267\u001b[0m, in \u001b[0;36m_spbase._process_toarray_args\u001b[1;34m(self, order, out)\u001b[0m\n\u001b[0;32m   1265\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m out\n\u001b[0;32m   1266\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1267\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m np\u001b[38;5;241m.\u001b[39mzeros(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mshape, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, order\u001b[38;5;241m=\u001b[39morder)\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 160. GiB for an array with shape (416809, 51666) and data type float64"
     ]
    }
   ],
   "source": [
    "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=tfidf_vectorizer.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3df3f70e-a305-42a4-9d33-6fc8cb3e9fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
